<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>crime</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Portfolio </strong>DS</a>
									<ul class="icons">
										<li><a href="https://www.linkedin.com/in/diana-carolina-rodriguez/" class="icon brands fa-linkedin" target="_blank" rel="noopener noreferrer"><span class="label">LinkedIn</span></a></li>			  
										<li><a href="https://github.com/dianarodriguezc" class="icon brands fa-github" target="_blank" rel="noopener noreferrer"><span class="label">GitHub</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<subsection>
									<header class="main">
										<h1>NLP Analysis of Tweets and Public Policy Documents for Public Policy Evaluation Terms in Colombia</h1>
									</header>
									<p>In this project, natural language processing (NLP) is used to identify and count terms related to the evaluation of public policies in CONPES (National Council for Economic and Social Policy) documents from Colombia for the period 2002-2021, as well as in the National Development Plans (PND) of Colombia for the period 2002-2022. The objective is to quantify the importance of public policy evaluations at the national level. Additionally, web scraping was performed using the Tweepy library to download tweets from previously selected government and academic accounts in order to evaluate the use of the same terms.</p>
									<p>The project involved the following steps:</p>

															<ol>
																<li>Reading, cleaning, identifying, and counting the terms of interest (Outcome evaluation, impact evaluation, cost-benefit analysis, cost-effectiveness analysis, etc.)</li>
																<li>Generating a normalized index of term counts per document, i.e., the average count of words related to public policy evaluations per document.</li>
																<li>Generating a contextual analysis by counting the unigrams and bigrams that occur around the initially sought terms.</li>
																<li>Performing web scraping using the Tweepy API to download tweets from selected accounts of interest, and then searching for and counting the terms of interest.</li>
															</ol>
								
									<hr class="Counting of terms by dimensions and generation of the normalized index of terms counts." />

									<h2>Counting of terms by dimensions and generation of the normalized index of terms counts.</h2>
									<p>In this step, several processes are generated to clean the text. These processes reduce the number of words that make up the entire universe of documents, which facilitates the automatic search and matching of the terms of interest within the documents.</p>
									<ol>
										<li><b>Document Reading:</b> In this first stage, all documents are read. Optical Character Recognition (OCR) is used to extract text from images or scanned documents.</li>
										<li><b>Text Cleaning:</b> In this stage, punctuation marks are removed, numbers are converted to lowercase, and stopwords are removed. Stopwords are words that do not have meaning on their own, such as articles, pronouns, and prepositions, as well as context-specific words that may be very frequent but do not add value. In this case, names of people, municipalities, and departments in Colombia are removed.</li>
										<li><b>Text Lemmatization:</b> In this step, all inflected forms of a word are transformed into its lemma or root. For example, words like "mesas," "mesón," and "mesitas" are all transformed into their lemma, "mesa."</li>
										<li><b>Phrase Search:</b> The document is divided into pages and then into smaller text fragments called analysis windows. Each analysis window consists of 250 words. In some cases, it may contain fewer words due to the number of words per page. The search for words/phrases is carried out in each of these fragments using regular expressions, and the count of each term or phrase is performed.</li>
									</ol>
									<p>As a result of this procedure, a table is generated containing the count of terms, structured as follows:</p>

									  				<div class="table-wrapper">
														<table class="alt">
															<thead>
																<tr>
																	<th>DIVIPOLA DOCUMENT CODE</th>
																	<th>TERM</th>
																	<th>COUNTING</th>																	
																</tr>
															</thead>
															<tbody>
																<tr>
																	<th>24</th>
																	<th>Institutional evaluation</th>
																	<th>20</th>
																</tr>
																<tr>
																	<th>...</th>
																	<th>...</th>
																	<th>...</th>
																	<th>...</th>																	<
																</tr>
																<tr>
																	<th>24</th>
																	<th>Evidence-based decision making</th>
																	<th>4</th>																	<
																</tr>
															</tbody>															
														</table>
													</div>

									<p>The number of terms related to public policy evaluation per year are graphed. In the case of the PND documents, to note a significant increase in the document for the period 2018-2022, with 139 words, compared to earlier periods. In the CONPES documents, to observe a higher number of words in the years 2019 to 2021.</p>
									<span class="image main"><img src="images/Conteo_evaluaciones.png" alt="" /></span>
									
									<p>The following graph shows the number of times each term was found in the documents. The terms with the highest number of occurrences are "evaluación de intervenciones/programas/política" with 407 appearances in the CONPES documents and 109 in the PND documents, followed by "evaluación de impacto" with 303 appearances in the CONPES documents and 109 in the PND documents.</p>
									<span class="image main"><img src="images/Conteo_term_evaluaciones.png" alt="" /></span>

									<p>After counting the terms of interest, an average of the number of terms per document is calculated for the CONPES documents, as there are multiple documents per year, unlike the PND, which corresponds to a single document per period. In 2019, 2020, and 2021, there are more terms on average with 5.8, 3.8, and 3.2 terms respectively.</p>
									<div style="text-align: center;"><img src="images/Ind_norm_evaluaciones.jpg" alt="" style="width: 45%; height: auto;"></div>
																		  
									
										<hr class="major" />

									<h2>Context analysis.</h2>
									<p>The contextual analysis involves examining the terms that surround the terms of interest. To do this, the windows (groups of 250 words) where the terms related to digital transformation were found are analyzed, and the other terms found in these windows are counted. The most frequent unigrams obtained were "nacional" for both types of documents, and the most frequent bigrams were "plan nacional" and "nacional planeación.</p>
									
									<span class="image main"><img src="images/Unigramas_evaluaciones.jpg" alt="" /></span>
									
									<span class="image main"><img src="images/bigramas_evaluaciones.jpg" alt="" /></span>
									
									<p>These analyses were generated at the term level, meaning that the words can be analyzed in context per term, per dimension, or per groups of terms. For this purpose, a table with the counts is generated with the following structure:</p>
									
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>DIVIPOLA DOCUMENT CODE</th>
													<th>TERM</th>
													<th>CONTEXT WORD</th>
													<th>COUNTING</th>																	
												</tr>
											</thead>
											<tbody>
												<tr>
													<th>24</th>
													<th>Impact evaluation</th>
													<th>Public policy</th>
													<th>20</th>
												</tr>
												<tr>
													<th>...</th>
													<th>...</th>
													<th>...</th>
													<th>...</th>																	<
												</tr>
												<tr>
													<th>24</th>
													<th>Institutional evaluation</th>
													<th>Higher Education</th>
													<th>4</th>																	<
												</tr>
											</tbody>															
										</table>
									</div>
									<h2>Scraping and Text Analysis of Tweets</h2>
									<p>In this step, tweets are downloaded from the accounts of interest from 2014 to 2021. The cleaning and preprocessing process is then performed, which includes removing special characters, cleaning stopwords, and lemmatization, similar to what was done with the documents. Once the tweets are prepared, a search is conducted for words related to the use of evaluations. This is done through frequency tables that show, for each tweet, the number of times each of the terms related to the use of evaluations appears.</p>
									<p>As a result, a database is generated containing 82 tweets that were published by the accounts of interest and that contain 118 terms related to the use of evaluations. For the counting, each of the words in the term is searched for, and it is counted if the tweet contains the words that make up the term, regardless of the order. The following graph shows the number of terms found in the tweets.</p>
									<div style="text-align: center;"><img src="images/Conteo_twitter.jpg" alt="" style="width: 100%; height: auto;"></div> <br>
																		
									<p><b>Full report:</b> To view full report and presentation click below (Available in Spanish).</p> 
									<ul class="actions">
										<li><a href="https://colaboracion.dnp.gov.co/CDT/Desarrollo%20Digital/UCD/Proyectos/2022_P19-Medicion_uso_evaluaciones/Informe_Final.pdf" class="button">View Full report</a></li>
										<li><a href="https://colaboracion.dnp.gov.co/CDT/Desarrollo%20Digital/UCD/Proyectos/2022_P19-Medicion_uso_evaluaciones/Presentación.pdf" class="button">View Presentation</a></li>
									</ul> 
									<hr>
								</section>
							</nav>
						</div>
					</div>


				<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Homepage</a></li>
									<li><a href="cv.html">About me</a></li>
										<span class="opener">Projects</span>
										<ul>
											<li><a href="crime.html">Crime prediction model</a></li>
											<li><a href="NLP_TD.html">NLP: Analyzing Digital Transformation in Colombia</a></li>
											<li><a href="ind_nec_jur.html">XGBoost Models for Predicting Legal Needs Indicators</a></li>
											<li><a href="NLP_Evaluaciones.html">NLP Analysis of Tweets and Public Policy Documents for Public Policy Evaluation Terms in Colombia</a></li>
										</ul>										
								</ul>
							</nav>

						<!-- Section -->
							<section>
								<header class="major">
									<h2>Contact me</h2>
								</header>
								<p>To contact me, you can write me an email.</p>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="#">dianacarolina400@gmail.com</a></li>
									<li class="icon solid fa-home">Montreal, QC, Canada</li>
								</ul>
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">&copy;  Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

					</div>
				</div>
		</nav>
	</body>
</html>